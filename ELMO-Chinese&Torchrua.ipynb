{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8615236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "import gc\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from torch.utils.data import Dataset,TensorDataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cc265aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chinese_embeddings(path, vocabulary, word2id, emb_size=300):\n",
    "    w_emb = np.zeros((len(word2id), emb_size))\n",
    "    with z.open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.decode('utf-8')\n",
    "            word = line.split()[0]\n",
    "                \n",
    "            if word in vocabulary: \n",
    "                try:\n",
    "                    emb = np.array(line.strip('\\n').split()[1:]).astype(np.float32)\n",
    "                    w_emb[word2id[word]] +=emb     \n",
    "                except:\n",
    "                    continue      \n",
    "    return w_emb   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1223628",
   "metadata": {},
   "source": [
    "# torchrua 真好用！！！用来pack. pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c0a13363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrua import reverse_packed_sequence, pad_packed_sequence, pack_padded_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "689ead6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ELMO, self).__init__()\n",
    "        \n",
    "        self.lamda = 1\n",
    "        self.layer_weights = torch.randn(3)\n",
    "        \n",
    "        self.word_embeds = 300\n",
    "        self.hidden_dim = 300\n",
    "        \n",
    "        self.lstm_forward_1 = nn.LSTM(self.word_embeds, self.hidden_dim,\n",
    "                            num_layers=1, bidirectional=False, batch_first = True)\n",
    "        self.lstm_forward_2 = nn.LSTM(self.word_embeds, self.hidden_dim,\n",
    "                            num_layers=1, bidirectional=False, batch_first = True)\n",
    "        self.lstm_backward_1 = nn.LSTM(self.word_embeds, self.hidden_dim,\n",
    "                            num_layers=1, bidirectional=False, batch_first = True)\n",
    "        self.lstm_backward_2 = nn.LSTM(self.word_embeds, self.hidden_dim,\n",
    "                            num_layers=1, bidirectional=False, batch_first = True)\n",
    "        \n",
    "    def forward(self,sequence):\n",
    "        \n",
    "        # Get the fixed embeddings for the input batch of sequences\n",
    "        embedding_sequence = self.get_embeddings(sequence)\n",
    "        # the list to store the real length of each input\n",
    "        lengths = []\n",
    "        for i in range(batch_size):\n",
    "            lengths.append(maskings[i,:].tolist().count(1))\n",
    "        # pack the sequence to let LSTM ignoring padding\n",
    "        pack_sequence = pack_padded_sequence(embedding_sequence, torch.Tensor(lengths).long(), batch_first=True)\n",
    "        reversed_pack_sequence = reverse_packed_sequence(pack_sequence)\n",
    "        # Randomly generate hidden_states and cell_states\n",
    "        h1 = torch.randn(1, batch_size, self.hidden_dim)\n",
    "        c1 = torch.randn(1, batch_size, self.hidden_dim)\n",
    "        \n",
    "        h2 = torch.randn(1, batch_size, self.hidden_dim)\n",
    "        c2 = torch.randn(1, batch_size, self.hidden_dim)\n",
    "        \n",
    "        h3 = torch.randn(1, batch_size, self.hidden_dim)\n",
    "        c3 = torch.randn(1, batch_size, self.hidden_dim)\n",
    "        \n",
    "        h4 = torch.randn(1, batch_size, self.hidden_dim)\n",
    "        c4 = torch.randn(1, batch_size, self.hidden_dim)\n",
    "        # Forward the input through 2-layer stacked and two directional LSTM, 1st layer\n",
    "        packed_forward_output_1 = self.lstm_forward_1(embedding_sequence,(h1,c1))[0]\n",
    "        packed_backward_output_1 = self.lstm_backward_1(torch.flip(embedding_sequence, dims=[1]),(h2,c2))[0]\n",
    "        # unpack and repack\n",
    "        forward_output_1_media = pad_packed_sequence(packed_forward_output_1, batch_first=True)\n",
    "        backward_output_1_media = pad_packed_sequence(packed_backward_output_1, batch_first=True)\n",
    "        forward_output_1 = pack_padded_sequence(forward_output_1_media, torch.Tensor(lengths).long(), batch_first=True)\n",
    "        backward_output_1 = pack_padded_sequence(backward_output_1_media, torch.Tensor(lengths).long(), batch_first=True)\n",
    "        # Residual adding\n",
    "        forward_output_1 = torch.add(forward_output_1,embedding_sequence)\n",
    "        backward_output_1 = torch.add(backward_output_1,torch.flip(embedding_sequence, dims=[1]))\n",
    "        # Forward the input through 2nd layer\n",
    "        packed_forward_output_2 = self.lstm_forward_2(forward_output_1,(h3,c3))[0]\n",
    "        packed_backward_output_2 = self.lstm_backward_2(backward_output_1,(h4,c4))[0]\n",
    "        # unpack\n",
    "        forward_output_2 = pad_packed_sequence(packed_forward_output_2, batch_first=True)\n",
    "        backward_output_2 = pad_packed_sequence(packed_backward_output_2, batch_first=True)\n",
    "        # Get the outputs from the first LSTM layer and the second LSTM layer\n",
    "        double_embedding = torch.cat((embedding_sequence,embedding_sequence),1)\n",
    "        firstLayer_output = torch.cat((forward_output_1_media, torch.flip(backward_output_1_media, dims=[1])), 1)\n",
    "        secondLayer_output = torch.cat((forward_output_2, torch.flip(backward_output_2, dims=[1])), 1)\n",
    "        # Get the weighted sum of different part of word representations\n",
    "        weights = nn.Softmax(self.layer_weights)\n",
    "        weighted_representation = weights[0]*double_embedding+weights[1]*firstLayer_output,weights[2]*secondLayer_output\n",
    "                \n",
    "        return (weighted_representation,(double_embedding, firstLayer_output, secondLayer_output))\n",
    "    \n",
    "    \n",
    "    def get_embeddings(self, sequence, embeddings):\n",
    "        \n",
    "        embeddings = torch.from_numpy(embeddings)\n",
    "        emb_size = embeddings.size()[1]\n",
    "        \n",
    "        batch_size = sequence.size()[0]\n",
    "        seq_length = sequence.size()[1]\n",
    "        \n",
    "        output = torch.zeros(batch_size,seq_length,emb_size)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_length):\n",
    "                w_id = sequence[i,j]\n",
    "                output[i,j,:]+= embeddings[w_id]\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b354a1",
   "metadata": {},
   "source": [
    "# Minor Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035dc8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
